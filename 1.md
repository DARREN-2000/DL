# Analysis of Your Deep Learning Framework Exercise

This exercise is asking you to build a foundational deep learning framework from scratch. Let me walk through each required component and provide implementation guidance.

## 1. Basic Optimizer (SGD)

You'll need to implement the Stochastic Gradient Descent optimizer in `Optimizers.py`:

```python
class Sgd:
    def __init__(self, learning_rate):
        self.learning_rate = learning_rate
        
    def calculate_update(self, weight_tensor, gradient_tensor):
        # Basic SGD update rule: w = w - learning_rate * gradient
        return weight_tensor - self.learning_rate * gradient_tensor
```

## 2. Base Layer

Implement `BaseLayer` in `Base.py` which all layers will inherit from:

```python
class BaseLayer:
    def __init__(self):
        self.trainable = False
        # You can add other members here as mentioned in the optional hint
```

## 3. Fully Connected Layer

For the `FullyConnected` layer in `FullyConnected.py`:

```python
import numpy as np
from Layers.Base import BaseLayer

class FullyConnected(BaseLayer):
    def __init__(self, input_size, output_size):
        super().__init__()
        self.trainable = True
        # Initialize weights uniformly random in range [0,1)
        # Including bias term in weights (input_size+1, output_size)
        self.weights = np.random.uniform(0, 1, (input_size + 1, output_size))
        self._optimizer = None
        self._gradient_weights = None
        self._input_tensor = None
        
    @property
    def optimizer(self):
        return self._optimizer
        
    @optimizer.setter
    def optimizer(self, optimizer):
        self._optimizer = optimizer
    
    @property
    def gradient_weights(self):
        return self._gradient_weights
        
    def forward(self, input_tensor):
        # Store input for backward pass
        self._input_tensor = input_tensor
        
        # Add bias term
        batch_size = input_tensor.shape[0]
        bias_column = np.ones((batch_size, 1))
        input_with_bias = np.hstack((input_tensor, bias_column))
        
        # y = x * W
        output = np.dot(input_with_bias, self.weights)
        return output
        
    def backward(self, error_tensor):
        # Calculate gradient for previous layer
        error_prev = np.dot(error_tensor, self.weights[:-1, :].T)
        
        # Calculate gradient for weights
        batch_size = self._input_tensor.shape[0]
        bias_column = np.ones((batch_size, 1))
        input_with_bias = np.hstack((self._input_tensor, bias_column))
        self._gradient_weights = np.dot(input_with_bias.T, error_tensor)
        
        # Update weights if optimizer is set
        if self._optimizer:
            self.weights = self._optimizer.calculate_update(self.weights, self._gradient_weights)
            
        return error_prev
```

## 4. ReLU Activation

For the `ReLU` layer in `ReLU.py`:

```python
import numpy as np
from Layers.Base import BaseLayer

class ReLU(BaseLayer):
    def __init__(self):
        super().__init__()
        self._input_tensor = None
        
    def forward(self, input_tensor):
        # Store input for backward pass
        self._input_tensor = input_tensor
        # ReLU function: max(0, x)
        return np.maximum(0, input_tensor)
        
    def backward(self, error_tensor):
        # Derivative of ReLU: 1 if x > 0, 0 otherwise
        return error_tensor * (self._input_tensor > 0)
```

## 5. SoftMax Layer

For the `SoftMax` layer in `SoftMax.py`:

```python
import numpy as np
from Layers.Base import BaseLayer

class SoftMax(BaseLayer):
    def __init__(self):
        super().__init__()
        self._prediction = None
        
    def forward(self, input_tensor):
        # For numerical stability, subtract max value
        shifted = input_tensor - np.max(input_tensor, axis=1, keepdims=True)
        exp_values = np.exp(shifted)
        self._prediction = exp_values / np.sum(exp_values, axis=1, keepdims=True)
        return self._prediction
        
    def backward(self, error_tensor):
        # Implement the backward pass for SoftMax
        # This is a bit complex due to the Jacobian matrix
        batch_size = error_tensor.shape[0]
        backward_tensor = np.zeros_like(error_tensor)
        
        for i in range(batch_size):
            y = self._prediction[i].reshape(-1, 1)
            jacobian = np.diagflat(y) - np.dot(y, y.T)
            backward_tensor[i] = np.dot(jacobian, error_tensor[i])
            
        return backward_tensor
```

## 6. Cross Entropy Loss

For the `CrossEntropyLoss` class in `Loss.py`:

```python
import numpy as np

class CrossEntropyLoss:
    def __init__(self):
        self._prediction_tensor = None
        
    def forward(self, prediction_tensor, label_tensor):
        self._prediction_tensor = prediction_tensor
        # Small epsilon to avoid log(0)
        epsilon = np.finfo(float).eps
        prediction_clipped = np.clip(prediction_tensor, epsilon, 1-epsilon)
        # Calculate cross entropy loss
        return -np.sum(label_tensor * np.log(prediction_clipped))
        
    def backward(self, label_tensor):
        # When combined with softmax, this simplifies to (y_pred - y_true)
        return self._prediction_tensor - label_tensor
```

## 7. Neural Network Skeleton

For the `NeuralNetwork` class in `NeuralNetwork.py`:

```python
import copy

class NeuralNetwork:
    def __init__(self, optimizer):
        self.optimizer = optimizer
        self.loss = []
        self.layers = []
        self.data_layer = None
        self.loss_layer = None
        
    def forward(self):
        input_tensor, label_tensor = self.data_layer.next()
        
        # Forward pass through all layers
        output = input_tensor
        for layer in self.layers:
            output = layer.forward(output)
            
        # Calculate loss
        loss_value = self.loss_layer.forward(output, label_tensor)
        
        return loss_value, label_tensor
        
    def backward(self, label_tensor):
        # Start backpropagation from the loss layer
        error_tensor = self.loss_layer.backward(label_tensor)
        
        # Backward pass through all layers in reverse order
        for layer in reversed(self.layers):
            error_tensor = layer.backward(error_tensor)
            
    def append_layer(self, layer):
        if layer.trainable:
            # Create a deep copy of the optimizer for each trainable layer
            layer.optimizer = copy.deepcopy(self.optimizer)
        self.layers.append(layer)
        
    def train(self, iterations):
        for i in range(iterations):
            loss_value, label_tensor = self.forward()
            self.loss.append(loss_value)
            self.backward(label_tensor)
            
    def test(self, input_tensor):
        # Forward pass for testing without updating weights
        output = input_tensor
        for layer in self.layers:
            output = layer.forward(output)
        return output
```

## Debugging and Testing

To run all tests:
```bash
python NeuralNetworkTests.py
```

To run a specific test:
```bash
python NeuralNetworkTests.py TestOptimizers1
```

To check for bonus points:
```bash
python NeuralNetworkTests.py Bonus
```

For submission, use the dispatch tool:
```bash
python dispatch.py -i ./src_to_implement -o submission.zip
```

These implementations should provide you with a solid foundation for passing all the tests. Make sure to thoroughly test each component individually before combining them.
